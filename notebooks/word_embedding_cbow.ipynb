{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "969a40d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/scratch/sagarsj42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c31e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848b642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW_Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super(CBOW_Dataset, self).__init__()\n",
    "        \n",
    "        self.reviews = list()\n",
    "        self.grouped_reviews = list()\n",
    "        self.data_pairs = list()\n",
    "        self.word_count = {'<UNK>': 0}\n",
    "        self.word_list = list()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        self.load_reviews()\n",
    "        self.group_reviews()\n",
    "        self.prepare_data_pairs()\n",
    "        self.compile_vocab()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        context_words = self.data_pairs[index][0]\n",
    "        word = self.data_pairs[index][1]\n",
    "        context_vector = self.encode_words(context_words)\n",
    "        target = self.encode_word(word)\n",
    "        \n",
    "        return (context_vector, target)\n",
    "    \n",
    "    def load_reviews(self):\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(1):\n",
    "            filename = 'review_words-' + str(i+1) + '.pkl'\n",
    "            print('Opening', filename, end='  ')\n",
    "            with open(filename, 'rb') as f:\n",
    "                reviews_set = pickle.load(f)\n",
    "                print('Contains', len(reviews_set), 'entries')\n",
    "                self.reviews.extend(reviews_set)\n",
    "\n",
    "        end = time.time()\n",
    "        print('Load data', 'Time taken:', end - start)\n",
    "        print('No. of reviews:', len(self.reviews))\n",
    "\n",
    "    def group_reviews(self):\n",
    "        start = time.time()\n",
    "\n",
    "        for review in self.reviews[:1]:\n",
    "            review_words = list()\n",
    "            for sentence in review:\n",
    "                review_words.extend(sentence)\n",
    "            if len(review_words) > 6:\n",
    "                self.grouped_reviews.append(review_words)\n",
    "            \n",
    "        end = time.time()\n",
    "        print('Grouping reviews', 'Time taken:', end - start)\n",
    "        print('No. of grouped reviews:', len(self.grouped_reviews))\n",
    "    \n",
    "    def prepare_data_pairs(self):\n",
    "        start = time.time()\n",
    "        for review in self.grouped_reviews:\n",
    "            for ind, word in enumerate(review):\n",
    "                win_size = min(ind, len(review)-ind-1, 3)\n",
    "\n",
    "                if win_size >= 3:\n",
    "                    left_ind = ind - win_size\n",
    "                    right_ind = ind + win_size\n",
    "                    context = list()\n",
    "                    \n",
    "                    for cont_i in range(left_ind, right_ind+1):\n",
    "                        if cont_i != ind:\n",
    "                            context.append(review[cont_i])\n",
    "                            \n",
    "                    self.data_pairs.append((context, word))\n",
    "        end = time.time()\n",
    "        print('Preparing data pairs', 'Time taken:', end - start)\n",
    "        print('No. of data pairs:', len(self.data_pairs))\n",
    "                    \n",
    "    def compile_vocab(self):\n",
    "        start = time.time()\n",
    "        \n",
    "        for review in self.reviews:\n",
    "            for sentence in review:\n",
    "                for word in sentence:\n",
    "                    if not word in self.word_count:\n",
    "                        self.word_count[word] = 1\n",
    "                    else:\n",
    "                        self.word_count[word] += 1\n",
    "\n",
    "        self.word_list = list(self.word_count.keys())\n",
    "        self.vocab_size = len(self.word_list)\n",
    "        print('Vocab size:', self.vocab_size)\n",
    "        end = time.time()\n",
    "        print('Preparing vocab', 'Time taken:', end - start)\n",
    "        \n",
    "    def encode_word(self, word):\n",
    "        return torch.tensor([self.word_list.index(word)], dtype=torch.long)\n",
    "    \n",
    "    def encode_words(self, words):\n",
    "        indices = [self.word_list.index(word) for word in words]\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "    \n",
    "    def decode_word(self, index):\n",
    "        return self.word_list[index.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f55bee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.activation_function = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.embeddings(inputs)\n",
    "        ct_size = out.shape[1]\n",
    "        out = (out.sum(1) / ct_size).view(out.shape[0], -1)\n",
    "        out = self.linear(out)\n",
    "        out = self.activation_function(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_embedding(self, word, vocab):\n",
    "        word_ind = torch.tensor(vocab.index(word), dtype=torch.long)\n",
    "        return self.embeddings(word_ind).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ed5f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening review_words-1.pkl  Contains 169913 entries\n",
      "Load data Time taken: 4.9263622760772705\n",
      "No. of reviews: 169913\n",
      "Grouping reviews Time taken: 1.33514404296875e-05\n",
      "No. of grouped reviews: 1\n",
      "Preparing data pairs Time taken: 0.00018787384033203125\n",
      "No. of data pairs: 139\n",
      "Vocab size: 33794\n",
      "Preparing vocab Time taken: 4.5156495571136475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CBOW_Dataset at 0x7f2716c6ddf0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CBOW_Dataset()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2fdd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f2716c4e160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=False)\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af9adf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {'vocab_size': dataset.vocab_size, 'embedding_dim': 750}\n",
    "model = CBOW(**model_args).cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "mbatch_losses = list()\n",
    "epoch_losses = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c494880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tStep 0\n",
      "Total epoch loss 10.417192459106445\n",
      "Epoch 1\n",
      "\tStep 0\n",
      "Total epoch loss 10.417078971862793\n",
      "Epoch 2\n",
      "\tStep 0\n",
      "Total epoch loss 10.416964530944824\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch', epoch)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i, (context_vector, target) in enumerate(dataloader):\n",
    "        if i % 100 == 0:\n",
    "            print('\\tStep', i)\n",
    "            if i % 100000 == 0 and i > 0:\n",
    "                torch.save({\n",
    "                    'vocab': dataset.word_list,\n",
    "                    'model_args': model_args,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'n_epochs': n_epochs,\n",
    "                    'mini_batch_losses': mbatch_losses,\n",
    "                    'epoch_losses': epoch_losses\n",
    "                },\n",
    "                    'checkpoint-'+str(epoch)+'-'+str(i)+'.pt')\n",
    "        \n",
    "        context_vector = context_vector.cuda()\n",
    "        target = target.view(-1).cuda()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        log_probs = model(context_vector)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        mbatch_losses.append(loss.item())\n",
    "    \n",
    "    epoch_losses.append(total_loss)\n",
    "    \n",
    "    torch.save({\n",
    "        'vocab': dataset.word_list,\n",
    "        'model_args': model_args,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'n_epochs': n_epochs,\n",
    "        'mini_batch_losses': mbatch_losses,\n",
    "        'epoch_losses': epoch_losses\n",
    "    },\n",
    "        'checkpoint-'+str(epoch)+'.pt')\n",
    "    \n",
    "    print('Total epoch loss', total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e5b2973",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'vocab': dataset.word_list,\n",
    "    'model_args': model_args,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'n_epochs': n_epochs,\n",
    "    'mini_batch_losses': mbatch_losses,\n",
    "    'epoch_losses': epoch_losses\n",
    "    },\n",
    "    'final-checkpoint.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "116471ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 33794\n",
      "Samples ['<UNK>', 'we', 'got', 'this', 'for']\n",
      "Model initialized CBOW(\n",
      "  (embeddings): Embedding(33794, 750)\n",
      "  (linear): Linear(in_features=750, out_features=33794, bias=True)\n",
      "  (activation_function): LogSoftmax(dim=-1)\n",
      ")\n",
      "Model loaded with trained weights OrderedDict([('embeddings.weight', tensor([[ 1.8629,  0.3649, -1.2000,  ..., -0.8849,  0.3445,  0.7882],\n",
      "        [-0.4234,  0.1914, -0.0266,  ..., -0.5505,  0.3514, -1.7271],\n",
      "        [ 0.4854, -0.2969, -0.9645,  ...,  1.0155, -0.1487, -0.6318],\n",
      "        ...,\n",
      "        [ 1.1696,  0.4288, -0.1578,  ..., -0.2005, -1.1478, -0.4725],\n",
      "        [ 0.0512,  1.9135,  1.0105,  ..., -0.1014, -0.4852,  1.3978],\n",
      "        [ 0.4521,  0.6331,  0.6027,  ..., -0.1792, -0.6109, -0.4757]])), ('linear.weight', tensor([[ 0.0273, -0.0003,  0.0269,  ...,  0.0087,  0.0031,  0.0293],\n",
      "        [ 0.0279,  0.0283, -0.0211,  ...,  0.0075,  0.0206, -0.0111],\n",
      "        [-0.0235,  0.0182,  0.0141,  ...,  0.0308,  0.0346,  0.0209],\n",
      "        ...,\n",
      "        [ 0.0143,  0.0158, -0.0220,  ..., -0.0089,  0.0024, -0.0018],\n",
      "        [ 0.0164,  0.0040, -0.0301,  ..., -0.0046, -0.0108,  0.0317],\n",
      "        [ 0.0360, -0.0128, -0.0025,  ..., -0.0277,  0.0297, -0.0355]])), ('linear.bias', tensor([ 0.0289,  0.0097,  0.0101,  ...,  0.0181, -0.0114,  0.0239]))])\n",
      "Epochs 3\n",
      "No. of mini-batch losses 3\n",
      "Samples [10.417192459106445, 10.417078971862793, 10.416964530944824]\n",
      "No. of epoch losses 3\n",
      "Samples [10.417192459106445, 10.417078971862793, 10.416964530944824]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('final-checkpoint.pt')\n",
    "\n",
    "vocab = checkpoint['vocab']\n",
    "print('Vocab size', len(vocab))\n",
    "print('Samples', vocab[:5])\n",
    "\n",
    "model_args = checkpoint['model_args']\n",
    "model = CBOW(**model_args)\n",
    "print('Model initialized', model)\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "print('Model loaded with trained weights', model.state_dict())\n",
    "\n",
    "n_epochs = checkpoint['n_epochs']\n",
    "print('Epochs', n_epochs)\n",
    "\n",
    "mini_batch_losses = checkpoint['mini_batch_losses']\n",
    "print('No. of mini-batch losses', len(mini_batch_losses))\n",
    "print('Samples', mini_batch_losses[:5])\n",
    "\n",
    "epoch_losses = checkpoint['epoch_losses']\n",
    "print('No. of epoch losses', len(epoch_losses))\n",
    "print('Samples', epoch_losses[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26d894b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 750])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-7.8090e-02, -2.7658e+00,  3.6753e-01,  3.1086e-01,  7.9372e-01,\n",
       "         -1.4315e+00, -9.5732e-01,  6.1610e-01, -7.3930e-01, -9.3644e-01,\n",
       "         -1.2328e-01, -2.5524e+00,  5.4905e-01,  8.0451e-01, -7.1114e-01,\n",
       "         -9.2626e-01,  6.0144e-01,  1.9739e-01,  1.7654e+00, -1.8216e-01,\n",
       "          1.5873e+00, -1.7611e+00, -3.8222e-01,  3.5357e-01, -2.2711e-01,\n",
       "         -7.4580e-01, -9.3559e-02, -3.8042e-02,  7.6754e-01,  3.3477e-01,\n",
       "         -3.4877e-03,  3.8403e-02,  1.1208e+00, -2.8794e+00, -3.6962e-01,\n",
       "          1.4453e+00,  7.9496e-01, -5.3998e-01,  1.4459e-01,  1.8286e-01,\n",
       "         -1.0667e+00,  1.2158e+00, -3.0689e+00, -1.0771e+00,  1.0395e+00,\n",
       "          6.2042e-01, -1.1133e+00, -8.1562e-02, -5.3052e-01, -1.1428e+00,\n",
       "          2.8851e-01, -9.6606e-01,  1.0011e+00, -1.1040e+00,  7.9581e-02,\n",
       "         -3.3222e-01,  1.6657e-01,  7.5044e-01, -2.1435e-01,  1.3358e+00,\n",
       "         -1.9062e+00,  6.4215e-01, -9.3439e-01,  1.4961e-01, -3.9431e-01,\n",
       "         -1.5163e+00, -9.5255e-01,  1.8413e+00, -3.4882e-01,  2.8716e-01,\n",
       "          7.5744e-01, -4.4595e-01, -6.9372e-01,  3.1017e-01, -4.5715e-01,\n",
       "          3.8458e-01,  1.3262e+00, -5.9705e-01,  2.4957e-01, -4.8934e-01,\n",
       "          6.1442e-01,  1.3676e+00, -6.7263e-01,  1.3907e+00, -1.5060e-01,\n",
       "         -6.4296e-02,  1.4108e+00, -1.0670e-01, -7.8460e-02,  1.5069e+00,\n",
       "         -3.8251e-01,  1.1667e+00, -3.3857e-01,  4.9839e-01, -2.3249e-01,\n",
       "          3.5352e-01,  4.6102e-01, -1.0970e+00, -1.2180e+00,  1.1471e+00,\n",
       "          1.0323e+00,  2.9414e-01,  1.6396e+00,  1.4104e-01,  3.8721e-02,\n",
       "         -5.0820e-01,  8.0522e-01, -3.7765e-02,  1.3792e+00, -4.9626e-01,\n",
       "         -1.2805e+00, -5.5430e-01, -5.6820e-01,  2.6944e-01, -5.1534e-01,\n",
       "          1.0243e+00,  2.0499e+00,  8.0566e-01,  1.6966e+00, -2.1560e-01,\n",
       "         -9.1693e-03,  3.8445e-01,  3.9856e-01,  9.3956e-01, -1.3737e+00,\n",
       "          1.0663e+00, -2.1255e+00, -2.4760e+00, -1.8838e+00, -2.4267e-01,\n",
       "         -3.9432e-02,  8.9410e-02,  4.7610e-01, -2.7957e-01,  8.3151e-01,\n",
       "          7.2949e-01, -2.7144e-02, -8.4211e-01, -6.6375e-01,  9.3296e-02,\n",
       "         -1.8535e+00, -3.5718e-01,  3.6974e-01, -1.8570e-02, -9.5774e-01,\n",
       "         -1.4208e+00, -2.8805e-01,  1.7785e+00,  1.0119e+00,  7.3203e-01,\n",
       "          5.5074e-01,  1.7277e-01, -5.3739e-01,  2.5101e-01, -4.6733e-01,\n",
       "          6.3322e-01,  1.2793e+00, -2.4455e+00,  1.3045e+00,  3.5123e-01,\n",
       "          9.2353e-01,  1.0079e+00,  1.1211e+00, -1.9789e+00, -3.0096e-01,\n",
       "         -1.4803e+00, -4.1735e-01, -1.4543e-02, -7.8788e-01, -5.2977e-01,\n",
       "          1.9641e-01, -5.4678e-01,  9.7501e-01, -1.0308e+00, -4.2266e-01,\n",
       "         -1.6802e+00, -1.3834e-01, -1.6907e+00,  1.3068e+00,  8.7967e-01,\n",
       "         -2.6631e-01, -5.7659e-03, -9.9387e-01, -1.6756e+00,  1.6696e+00,\n",
       "          4.5938e-02,  2.4768e-01, -8.7851e-01,  8.3808e-01, -6.8183e-01,\n",
       "          5.7550e-01, -7.5962e-01, -1.6863e+00,  6.1065e-01,  1.3936e+00,\n",
       "          7.5958e-01, -7.4685e-01,  2.6775e-01,  6.0810e-01,  1.4321e+00,\n",
       "         -2.1718e+00, -1.9807e+00, -3.5825e-01,  3.7442e-01, -1.8319e-01,\n",
       "         -5.9911e-01, -3.8494e-01,  1.1717e-01, -1.1034e+00, -1.3521e+00,\n",
       "         -2.3189e-01, -6.4494e-01, -1.4244e-01, -1.8495e+00, -1.3066e+00,\n",
       "          1.3033e+00,  6.5464e-01, -4.5778e-01,  5.5722e-01,  1.0160e+00,\n",
       "          1.1961e+00,  1.4516e+00, -5.9138e-01,  4.7066e-01, -2.5744e-01,\n",
       "          4.2275e-01,  8.3295e-02, -1.3034e+00,  2.0283e+00,  1.9957e+00,\n",
       "          1.2455e+00, -1.7188e-01,  2.0096e+00,  5.0313e-01, -9.4875e-01,\n",
       "         -9.4601e-02,  1.8981e+00, -1.0347e+00, -8.5898e-01, -1.8115e+00,\n",
       "         -2.2823e-01,  1.2024e-01, -1.2152e+00,  6.4409e-01,  5.4008e-01,\n",
       "          9.1356e-01,  3.9643e-01, -1.0686e+00,  4.7434e-01,  1.5226e-01,\n",
       "         -8.8342e-02, -1.2556e+00,  1.3570e+00,  9.3106e-01, -6.2617e-01,\n",
       "         -1.1971e+00, -2.1987e-01,  1.0440e+00,  5.8431e-01,  7.6812e-01,\n",
       "         -1.3706e-01,  1.8709e-01, -2.1570e-01, -7.1030e-01, -1.2433e+00,\n",
       "         -6.5700e-02, -5.1689e-01, -6.0706e-01,  2.2577e-01,  1.0028e+00,\n",
       "         -1.1318e+00, -4.2872e-01,  2.2802e-01,  3.9039e-01,  2.5455e-01,\n",
       "         -7.4698e-01,  2.3740e+00, -2.1701e-01,  6.5236e-01,  4.8498e-02,\n",
       "         -1.6999e+00, -1.3549e+00,  5.2386e-01, -5.1155e-01,  4.1615e-01,\n",
       "         -1.0216e+00, -6.3679e-01,  8.3433e-01,  1.8468e-01,  1.3225e+00,\n",
       "          3.1482e-01, -6.0758e-01, -8.7553e-01,  2.3110e-01, -1.2043e+00,\n",
       "         -9.8618e-01,  6.5765e-01,  1.3065e+00,  5.7137e-01,  7.8614e-01,\n",
       "          1.6692e-01, -1.2877e+00,  4.0787e-01,  1.3154e-01, -7.5045e-01,\n",
       "         -8.9747e-01,  1.0494e+00,  1.2517e+00, -7.4802e-01,  1.2593e+00,\n",
       "         -2.6920e-01,  4.0046e-01, -5.0154e-01,  7.5473e-01,  1.5913e+00,\n",
       "          6.3952e-01, -5.6288e-01,  9.9157e-02, -6.8505e-01,  8.3225e-03,\n",
       "         -6.2830e-01,  4.1588e-01,  2.4604e-01,  4.5744e-01, -1.8666e+00,\n",
       "          2.0998e+00, -7.5328e-01,  1.4154e+00,  7.8856e-01, -5.3132e-02,\n",
       "         -1.1433e-01,  1.8948e+00,  3.3298e-01, -3.9680e-01, -5.9404e-01,\n",
       "         -1.4649e+00, -4.7611e-01, -4.8463e-01,  8.0369e-01, -2.2124e+00,\n",
       "         -9.2210e-01,  1.5490e+00, -1.5253e+00, -1.5268e-01,  3.5264e-01,\n",
       "          1.0615e+00, -3.5038e-01,  4.4709e-01, -8.6690e-01,  3.1968e-01,\n",
       "          7.2848e-01, -2.5962e+00,  8.6506e-01, -3.3470e-01,  4.0246e-01,\n",
       "         -8.3515e-01,  6.1236e-01, -2.2967e+00,  4.4827e-01,  3.3605e-01,\n",
       "          1.3374e-01, -4.0770e-01, -6.4802e-01, -4.3879e-01,  1.2131e+00,\n",
       "         -9.5609e-01, -6.1969e-01, -9.4166e-01,  1.0125e+00, -1.0832e+00,\n",
       "          2.2121e+00, -4.7064e-01, -9.3377e-01, -2.0222e+00,  6.2778e-01,\n",
       "          1.0297e+00,  7.4914e-01, -9.1419e-01,  1.5619e+00, -2.4452e-01,\n",
       "          4.9088e-01, -8.7918e-01, -1.2593e+00, -6.9875e-03,  6.2495e-01,\n",
       "          1.1782e+00,  2.4566e-01,  1.5359e+00,  4.7987e-01, -1.8893e-01,\n",
       "         -8.9323e-01, -5.8144e-02, -3.6713e-01, -3.0372e-01, -1.4910e+00,\n",
       "          1.2660e+00, -3.1139e-01,  1.4911e+00, -1.3049e-01, -4.0474e-03,\n",
       "          3.3560e-02, -1.8571e+00, -1.8326e-01, -1.1392e+00, -1.9621e+00,\n",
       "         -1.5554e+00, -4.1446e-01, -1.2832e+00,  1.6613e-01, -1.0947e+00,\n",
       "         -1.1765e+00,  1.2608e+00, -1.3755e-02,  1.8171e+00, -2.4575e-01,\n",
       "          4.1710e-01,  6.2480e-01,  5.0621e-01,  1.1339e+00,  3.9789e-01,\n",
       "         -9.6235e-01,  3.1727e-02,  2.7670e-01, -5.4336e-01,  6.9829e-01,\n",
       "         -6.4748e-01,  1.1811e+00,  1.3714e+00,  1.0532e+00, -4.3124e-01,\n",
       "         -5.5055e-01,  2.2475e-01,  2.1033e+00,  2.2044e-01,  3.6823e-01,\n",
       "         -3.3394e-01,  1.2359e-01,  6.7957e-01, -4.6452e-01, -1.7478e+00,\n",
       "          1.2537e+00, -2.2765e+00, -7.8236e-01,  1.6305e-01, -6.4590e-02,\n",
       "         -5.3007e-01,  2.1593e-01, -7.7674e-01,  1.0580e+00, -1.4697e+00,\n",
       "          1.0539e+00, -9.0559e-03,  1.7917e+00, -1.8114e-01, -1.3254e+00,\n",
       "          8.5616e-01, -2.4013e+00,  1.4795e+00,  1.0253e+00,  5.5857e-01,\n",
       "         -3.9580e-01, -5.2583e-01, -1.1122e+00, -4.5557e-01, -1.7113e+00,\n",
       "         -1.1243e+00,  1.0822e+00, -3.6327e-01, -2.9701e-02,  2.7250e-01,\n",
       "         -6.9038e-01, -3.5909e-01,  9.3930e-01,  1.2211e+00,  3.3073e-01,\n",
       "         -1.2944e+00,  7.7325e-01,  1.0891e+00,  7.6064e-03, -3.5934e-01,\n",
       "          1.6772e+00,  4.7602e-01,  5.7666e-01, -1.0209e+00, -5.4203e-01,\n",
       "          1.5608e+00, -6.0526e-01,  4.9408e-01, -2.2457e+00, -7.6331e-01,\n",
       "         -6.8132e-01, -3.2743e-01,  1.8261e+00, -8.3626e-01, -4.0546e-02,\n",
       "         -4.6069e-01,  2.5790e-01, -5.4937e-01, -4.4073e-01,  9.7754e-01,\n",
       "          5.2369e-01,  4.3405e-02,  1.4762e-01,  4.7220e-03,  6.1019e-01,\n",
       "         -1.5185e+00,  1.5611e+00,  1.6105e+00, -9.2866e-01,  3.3247e-01,\n",
       "          9.0809e-02, -5.0528e-01, -1.2178e+00, -6.7866e-01, -1.0635e+00,\n",
       "          1.1896e+00, -2.8033e-02,  7.8627e-01, -5.0812e-01, -1.5528e+00,\n",
       "         -2.8311e-01, -9.6394e-01,  8.1969e-01, -4.3227e-02,  4.3130e-01,\n",
       "          8.7180e-01,  3.2453e-01, -1.1808e+00, -1.0123e+00, -9.8370e-01,\n",
       "          4.6152e-02,  7.9989e-01,  1.4869e-02, -1.2200e+00,  2.3085e+00,\n",
       "         -1.4064e+00,  1.1132e-01,  6.4612e-01,  3.0909e-01, -6.6597e-01,\n",
       "         -8.3462e-01, -6.1760e-01,  1.5089e-01,  2.5916e-01,  2.3255e-02,\n",
       "         -3.9140e-01,  1.0178e+00, -2.3387e-01,  8.0231e-01,  1.5042e+00,\n",
       "          4.8296e-01, -9.2590e-01,  7.1572e-01,  4.1651e-01,  2.0954e-03,\n",
       "         -1.4980e+00,  2.8941e+00,  1.9208e+00, -8.6574e-01, -7.6939e-01,\n",
       "         -6.7349e-02,  2.5592e-02,  9.2626e-02,  7.0338e-01, -8.2430e-01,\n",
       "          1.2265e+00,  5.1418e-01, -1.3388e+00,  2.3240e-02, -8.6073e-01,\n",
       "          1.4327e-01, -1.2497e-01, -1.7908e+00, -7.4232e-01, -4.0963e-01,\n",
       "         -3.4682e-01, -7.2157e-01,  9.9512e-01, -4.5581e-01,  1.4524e-01,\n",
       "         -1.2016e-01, -4.7576e-01,  1.1164e+00,  8.0784e-01,  1.7922e+00,\n",
       "         -4.8469e-01,  1.7779e-02, -4.2260e-02, -8.0575e-01, -1.5614e+00,\n",
       "          9.9343e-01, -1.3990e-01,  6.2517e-01, -7.7790e-01,  2.7943e-01,\n",
       "         -3.2922e-01, -1.4260e+00, -1.0671e+00, -1.8542e-01,  1.4968e+00,\n",
       "          1.6136e+00,  8.1086e-01, -5.4629e-01, -8.8703e-01,  1.3322e-01,\n",
       "          2.9843e-01, -9.5751e-01,  1.4416e+00,  1.9039e-01, -9.3916e-01,\n",
       "          2.0805e-01,  1.2061e+00, -6.2578e-01, -2.2941e-01, -1.6966e-01,\n",
       "          1.0400e+00, -5.2937e-01, -9.9339e-02,  1.3647e+00, -1.2613e+00,\n",
       "         -2.3278e+00, -1.1946e+00,  2.4886e-01,  6.3991e-01, -1.7810e+00,\n",
       "         -2.3501e+00,  9.2937e-01,  1.6849e+00,  2.7727e-01,  8.5209e-01,\n",
       "          1.1706e-01,  1.1240e+00, -4.0987e-01, -2.0254e+00, -6.4959e-01,\n",
       "         -7.9702e-02,  5.3800e-01, -1.5312e+00,  1.3503e+00, -8.1135e-01,\n",
       "          1.4927e-01, -1.7254e+00, -8.6769e-01, -1.0875e-01,  1.1986e+00,\n",
       "         -2.4549e-01, -1.8271e+00, -8.7417e-01, -9.2161e-01, -1.0289e-01,\n",
       "         -7.5870e-01, -1.3541e+00,  1.7018e+00, -3.5379e-01,  9.0906e-01,\n",
       "          8.2193e-01,  4.0364e-01, -1.3709e-01, -6.6787e-01,  1.1361e+00,\n",
       "          7.0177e-02, -1.6667e+00,  1.4441e+00, -6.5192e-01, -1.2136e+00,\n",
       "          3.4684e-01, -6.1010e-01, -3.1989e-01,  5.0582e-01, -1.6603e+00,\n",
       "         -1.3978e+00,  1.1821e+00,  4.3753e-01,  2.1953e+00,  4.0677e-01,\n",
       "          1.2644e-01,  1.0010e+00,  7.0082e-01, -1.2658e+00,  1.3814e+00,\n",
       "          4.0813e-01, -2.2104e+00,  1.2347e+00, -6.4537e-01, -1.2778e-01,\n",
       "          2.4989e+00, -1.0065e-01, -1.5581e+00,  1.4901e+00, -7.4888e-01,\n",
       "         -1.0637e+00,  5.3810e-01,  1.0273e+00,  9.9881e-01,  6.1939e-01,\n",
       "         -3.1645e-02,  9.7012e-01,  3.0712e-01,  8.0169e-02, -6.9526e-01,\n",
       "          1.3664e-01,  2.1224e-01,  5.2141e-02,  8.8084e-01,  3.0945e-01,\n",
       "          1.2550e-01, -5.8319e-02,  4.0900e-01,  1.4021e+00, -1.8223e-01,\n",
       "          3.0566e-01,  7.5736e-01,  1.3510e+00,  1.1737e+00, -4.4578e-01,\n",
       "          6.6892e-01, -9.4011e-02,  7.2414e-01, -1.5307e+00,  6.6529e-01,\n",
       "          4.5705e-02,  1.6579e+00, -2.4012e+00,  1.4233e+00,  6.4910e-01,\n",
       "         -5.3438e-02,  5.1834e-01, -2.8692e-02, -3.6092e-01,  1.0433e+00,\n",
       "          6.4872e-03, -3.0863e-01,  8.3917e-02,  1.0016e+00,  1.0883e+00,\n",
       "          2.2587e-01,  7.7051e-01, -3.1145e-02, -1.5675e+00,  3.0062e-01,\n",
       "          1.5467e-01, -6.4872e-01, -3.6549e-01,  8.8602e-02,  4.6094e-01,\n",
       "          1.5615e+00, -1.8735e+00,  5.2771e-01,  2.0091e-01, -6.5121e-02]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = model.get_embedding('by', vocab).detach()\n",
    "print(embed.shape)\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae5851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
