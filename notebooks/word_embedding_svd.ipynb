{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab94f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /scratch/sagarsj42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f07a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/sagarsj42'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/scratch/sagarsj42')\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7107c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "import enchant\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f36a92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./rarity-checked/review.words.unchecked.rarity.1.pkl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_vocab = False\n",
    "remove_rare = True\n",
    "filename = 'review.words.unchecked.rarity'\n",
    "store_dir = os.path.join('.', 'rarity-checked')\n",
    "\n",
    "with open('rare_words.pkl', 'rb') as f:\n",
    "    rare_words = set(pickle.load(f))\n",
    "\n",
    "os.path.join(store_dir, filename + '.1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408dbea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1689188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['We got this GPS for my husband who is an (OTR) over the road trucker.  Very Impressed with the shipping time, it arrived a few days earlier than expected...  within a week of use however it started freezing up... could of just been a glitch in that unit.  Worked great when it worked!  Will work great for the normal person as well but does have the \"trucker\" option. (the big truck routes - tells you when a scale is coming up ect...)  Love the bigger screen, the ease of use, the ease of putting addresses into memory.  Nothing really bad to say about the unit with the exception of it freezing which is probably one in a million and that\\'s just my luck.  I contacted the seller and within minutes of my email I received a email back with instructions for an exchange! VERY impressed all the way around!',\n",
       " 'I\\'m a professional OTR truck driver, and I bought a TND 700 at a truck stop hoping to make my life easier.  Rand McNally, are you listening?First thing I did after charging it was connect it to my laptop and install the software and then attempt to update it.  The software detected a problem with my update and wanted my home address so I could be sent a patch on an SD card.  Hello?  I don\\'t think I\\'m all that unusual; my home address is a PO box that a friend checks weekly and that I might get to check every six months or so.  I live in my truck and at truck stops.  If you need to make a patch available on an SD card then you should send the SD cards to the truck stops where the devices are sold.  I ran the update program multiple times until the program said that the TND 700 was completely updated.I programmed in the height (13\\'6\"), the length (53\\') and the weight (80,000#) of my rig and told it that I preferred highways.  I was parked at a truck stop in the Cincinnati OH area.  My next pickup was about 15 miles down the same freeway but on the other side of it a couple of blocks.  My cell phone GPS (Sprint) said to get on the freeway to get to my pickup.  The TND 700 routed me thru 23 miles of residential streets before finally getting me to my pickup.  Very exciting, especially since every time I refused to turn down a street posted \"No Trucks\" the TND 700 took almost 5 minutes to figure a re-route, and it happened multiple times on that short trip.I decided to give it another chance.  After my pickup on the north side of Cincinnati just off of I-75 I needed to head to Phoenix AZ via I-71.  Easy route is to just hop on I-75 and drive west and south to the intersection of I-71.  Indeed, that is what my cell phone advised.  The TND 700, however, wanted to route me over surface streets across the city and pick up I-75 on the other side of the city.  I turned it off and the next time I passed a truck stop of the same chain I purchased it at I returned it and got my money back.I then spent $30 on a cheap printer.  Now I take a minute to set up my route on Google and print it out.  Hasn\\'t gotten me lost yet over several cross country trips.',\n",
       " \"Well, what can I say.  I've had this unit in my truck for about four days now.  Prior to that I had a Garmin 755T non-truck GPS.  One of my favorite features in that unit was the ability to plan a route by determining mileage using the stop or via feature.  What I would do is using a map I would route myself several different ways forcing the unit by putting in stops or vias at different locations along the route, otherwise, like most GPS 's, it determines what it thinks is the best route.  I could add up to 10 Via's or stop points for each route and then based on mileage and other factors determine which is the best route to take.  Multiple stops and the ability to route was the most important reason for having the Garmin.  However it was not truck specific.  And considering I am now hauling strictly hazmat I wanted something that would take that into consideration.  After perusing various forums, review sites, and word-of-mouth my choices boiled down to the Garmin 465T or the RAND McNally Intelliroute TND 700.  Even though it was quite a bit more than the Garmin I chose the TND 700 for several reasons.  The main one being the extra screen size, and its ability to coordinate with the RAND McNally truck atlas and also its ease of updating.Now on to my first impression of the TND 700.  It seems to be an aesthetically pleasing and durably built unit.  The first thing I noticed was it's very slow to boot compared to my old Garmin.  Whether this is unique to the TND 700 or is common amongst all truck specific gps units I cannot tell, but it's really not that big of a deal.  The second thing I noticed was the overwhelming wealth of information put forth.  That might explain why the manual (available via the TND dock) is well over 100 pages long.....  There is somewhat of a learning curve with this unit.  The next thing I noticed was the complexity of entering routes.  As previously mentioned I like to force it into my preferred routing by the use of stops or vias.  That was a big no go with this unit.  While you can enter multiple stops or vias it is nowhere near as user-friendly as my old Garmin.  Furthermore there is no way you can determine total mileage on the route that you have chosen, as I could with the Garmin.  Totally flummoxed by what appeared to be an omission of one of the best routing tools a trucker could have I went online and verified through an expert source connected with RAND McNally that no, that feature was a couple updates down the line and was not available at this point.  Unbelievable.  Also forcing the unit to follow a specific route can be very challenging.  For a unit of this price, and feature laden, I find this totally unacceptable.  I am still mulling over selling this unit  and buying the Garmin 465T.  I really do like this GPS, the screen is magnificent, and the volume is awesome.Another thing I've noticed, which I do not think is unique to this unit, is some of the weird routing that it does.  I've never owned a truck specific GPS before but after playing with this one for a couple of days I get the impression that what RAND MacNally (and the others that use Navteq) has done is take a plain old car specific Navteq map and a road atlas with truck restrictions and made notifications on the map.  In most databases there is not a 100% thorough listing of every road in the country.  What I mean is that this unit will route you down roads you don't belong on.  Today while coming home it tried routing me on several  9 ton County roads.  What that means is that a truck is limited to 73280 unless the road is posted at 10 tons.  County roads do not have that restriction listed in the RAND McNally road atlas, so I believe that is why it is not listed.....  Also unacceptable in a unit of this price and sold as truck specific.  However I'd be willing to bet the other truck specific units are the same way as no one has ever done a truck specific version of NavTeq.Another bone I have to pick is the POI's. Not only the truck specific ones but the others all seem to be somewhat outdated.  I think the Garmin, at least the ones that I have used, are more current.  It would also be nice if I could add my own POI'sTo sum it up I will keep this unit a little longer to see if I can make it workable.  If not I will go back to Garmin.  I like the concept and I like the unit but at this point I'll have to say I am somewhat unimpressed.pros:Large screenterrific imagesoutstanding volumecons:inability to determine route mileage by multiple stops and viasa somewhat outdated POI information.Non-truck specific routings.Cost.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = list()\n",
    "\n",
    "for json_line in open('reviews_Electronics_5.json'):\n",
    "    reviews_json = json.loads(json_line)\n",
    "    reviews.append(reviews_json['reviewText'])\n",
    "\n",
    "print(len(reviews))\n",
    "reviews[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a9900e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['we',\n",
       "  'got',\n",
       "  'this',\n",
       "  'gps',\n",
       "  'for',\n",
       "  'my',\n",
       "  'husband',\n",
       "  'who',\n",
       "  'is',\n",
       "  'an',\n",
       "  'otr',\n",
       "  'over',\n",
       "  'the',\n",
       "  'road',\n",
       "  'trucker'],\n",
       " ['very',\n",
       "  'impressed',\n",
       "  'with',\n",
       "  'the',\n",
       "  'shipping',\n",
       "  'time',\n",
       "  'it',\n",
       "  'arrived',\n",
       "  'a',\n",
       "  'few',\n",
       "  'days',\n",
       "  'earlier',\n",
       "  'than',\n",
       "  'expected',\n",
       "  'within',\n",
       "  'a',\n",
       "  'week',\n",
       "  'of',\n",
       "  'use',\n",
       "  'however',\n",
       "  'it',\n",
       "  'started',\n",
       "  'freezing',\n",
       "  'up',\n",
       "  'could',\n",
       "  'of',\n",
       "  'just',\n",
       "  'been',\n",
       "  'a',\n",
       "  'glitch',\n",
       "  'in',\n",
       "  'that',\n",
       "  'unit'],\n",
       " ['worked', 'great', 'when', 'it', 'worked'],\n",
       " ['will',\n",
       "  'work',\n",
       "  'great',\n",
       "  'for',\n",
       "  'the',\n",
       "  'normal',\n",
       "  'person',\n",
       "  'as',\n",
       "  'well',\n",
       "  'but',\n",
       "  'does',\n",
       "  'have',\n",
       "  'the',\n",
       "  'trucker',\n",
       "  'option'],\n",
       " ['the',\n",
       "  'big',\n",
       "  'truck',\n",
       "  'routes',\n",
       "  'tells',\n",
       "  'you',\n",
       "  'when',\n",
       "  'a',\n",
       "  'scale',\n",
       "  'is',\n",
       "  'coming',\n",
       "  'up',\n",
       "  'ect',\n",
       "  'love',\n",
       "  'the',\n",
       "  'bigger',\n",
       "  'screen',\n",
       "  'the',\n",
       "  'ease',\n",
       "  'of',\n",
       "  'use',\n",
       "  'the',\n",
       "  'ease',\n",
       "  'of',\n",
       "  'putting',\n",
       "  'addresses',\n",
       "  'into',\n",
       "  'memory'],\n",
       " ['nothing',\n",
       "  'really',\n",
       "  'bad',\n",
       "  'to',\n",
       "  'say',\n",
       "  'about',\n",
       "  'the',\n",
       "  'unit',\n",
       "  'with',\n",
       "  'the',\n",
       "  'exception',\n",
       "  'of',\n",
       "  'it',\n",
       "  'freezing',\n",
       "  'which',\n",
       "  'is',\n",
       "  'probably',\n",
       "  'one',\n",
       "  'in',\n",
       "  'a',\n",
       "  'million',\n",
       "  'and',\n",
       "  'thats',\n",
       "  'just',\n",
       "  'my',\n",
       "  'luck'],\n",
       " ['i',\n",
       "  'contacted',\n",
       "  'the',\n",
       "  'seller',\n",
       "  'and',\n",
       "  'within',\n",
       "  'minutes',\n",
       "  'of',\n",
       "  'my',\n",
       "  'email',\n",
       "  'i',\n",
       "  'received',\n",
       "  'a',\n",
       "  'email',\n",
       "  'back',\n",
       "  'with',\n",
       "  'instructions',\n",
       "  'for',\n",
       "  'an',\n",
       "  'exchange'],\n",
       " ['very', 'impressed', 'all', 'the', 'way', 'around']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tok = nltk.tokenize.TweetTokenizer()\n",
    "en_dict = enchant.Dict('en_US')\n",
    "review_words = list()\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    if i <= 170000:\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        sent_list = list()\n",
    "        sents = sent_detector.tokenize(review)\n",
    "        for sent in sents:\n",
    "            word_list = list()\n",
    "            words = word_tok.tokenize(sent)\n",
    "            for word in words:\n",
    "                word = re.sub(r'[^\\w\\s]|[0-9]', '', word).lower().strip()\n",
    "                if word:\n",
    "                    if check_vocab and en_dict.check(word):\n",
    "                        word_list.append(word)\n",
    "                    elif remove_rare and word not in rare_words:\n",
    "                        word_list.append(word)\n",
    "                    else:\n",
    "                        word_list.append(word)\n",
    "            if word_list:\n",
    "                sent_list.append(word_list)\n",
    "        if sent_list:\n",
    "            review_words.append(sent_list)\n",
    "            \n",
    "with open(os.path.join(store_dir, filename + '.1.pkl'), 'wb') as f:\n",
    "    pickle.dump(review_words, f)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.1.pkl'), 'rb') as f:\n",
    "    r = pickle.load(f)\n",
    "    \n",
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2249ae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['paid',\n",
       "  'for',\n",
       "  'some',\n",
       "  'microsoft',\n",
       "  'points',\n",
       "  'got',\n",
       "  'a',\n",
       "  'code',\n",
       "  'and',\n",
       "  'transferred',\n",
       "  'it',\n",
       "  'to',\n",
       "  'my',\n",
       "  'xbox',\n",
       "  'live',\n",
       "  'account',\n",
       "  'in',\n",
       "  'just',\n",
       "  'a',\n",
       "  'few',\n",
       "  'minutes'],\n",
       " ['didnt',\n",
       "  'have',\n",
       "  'to',\n",
       "  'pay',\n",
       "  'tax',\n",
       "  'at',\n",
       "  'a',\n",
       "  'local',\n",
       "  'store',\n",
       "  'and',\n",
       "  'now',\n",
       "  'microsoft',\n",
       "  'doesnt',\n",
       "  'have',\n",
       "  'any',\n",
       "  'of',\n",
       "  'my',\n",
       "  'cc',\n",
       "  'information'],\n",
       " ['amazon', 'rocks']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_words = list()\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    if i > 170000 and i < 400000:\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        sent_list = list()\n",
    "        sents = sent_detector.tokenize(review)\n",
    "        for sent in sents:\n",
    "            word_list = list()\n",
    "            words = word_tok.tokenize(sent)\n",
    "            for word in words:\n",
    "                word = re.sub(r'[^\\w\\s]|[0-9]', '', word).lower().strip()\n",
    "                if word:\n",
    "                    if check_vocab and en_dict.check(word):\n",
    "                        word_list.append(word)\n",
    "                    elif remove_rare and word not in rare_words:\n",
    "                        word_list.append(word)\n",
    "                    else:\n",
    "                        word_list.append(word)\n",
    "            if word_list:\n",
    "                sent_list.append(word_list)\n",
    "        if sent_list:\n",
    "            review_words.append(sent_list)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.2.pkl'), 'wb') as f:\n",
    "    pickle.dump(review_words, f)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.2.pkl'), 'rb') as f:\n",
    "    r = pickle.load(f)\n",
    "    \n",
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d08c6e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['when',\n",
       "  'using',\n",
       "  'this',\n",
       "  'cable',\n",
       "  'to',\n",
       "  'connect',\n",
       "  'my',\n",
       "  'phone',\n",
       "  'to',\n",
       "  'my',\n",
       "  'computer',\n",
       "  'windows',\n",
       "  'does',\n",
       "  'not',\n",
       "  'recognize',\n",
       "  'it',\n",
       "  'whereas',\n",
       "  'it',\n",
       "  'does',\n",
       "  'for',\n",
       "  'other',\n",
       "  'cablesupdate',\n",
       "  'i',\n",
       "  'further',\n",
       "  'discovered',\n",
       "  'the',\n",
       "  'the',\n",
       "  'cable',\n",
       "  'does',\n",
       "  'not',\n",
       "  'carry',\n",
       "  'a',\n",
       "  'charging',\n",
       "  'current',\n",
       "  'either'],\n",
       " ['my',\n",
       "  'phone',\n",
       "  'shows',\n",
       "  'that',\n",
       "  'its',\n",
       "  'charging',\n",
       "  'but',\n",
       "  'the',\n",
       "  'battery',\n",
       "  'drains',\n",
       "  'as',\n",
       "  'normal'],\n",
       " ['other', 'cables', 'work', 'properly']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_words = list()\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    if i >= 600000 and i < 800000:\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        sent_list = list()\n",
    "        sents = sent_detector.tokenize(review)\n",
    "        for sent in sents:\n",
    "            word_list = list()\n",
    "            words = word_tok.tokenize(sent)\n",
    "            for word in words:\n",
    "                word = re.sub(r'[^\\w\\s]|[0-9]', '', word).lower().strip()\n",
    "                if word:\n",
    "                    if check_vocab and en_dict.check(word):\n",
    "                        word_list.append(word)\n",
    "                    elif remove_rare and word not in rare_words:\n",
    "                        word_list.append(word)\n",
    "                    else:\n",
    "                        word_list.append(word)\n",
    "            if word_list:\n",
    "                sent_list.append(word_list)\n",
    "        if sent_list:\n",
    "            review_words.append(sent_list)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.3.pkl'), 'wb') as f:\n",
    "    pickle.dump(review_words, f)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.3.pkl'), 'rb') as f:\n",
    "    r = pickle.load(f)\n",
    "    \n",
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33c541ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'bought',\n",
       "  'this',\n",
       "  'to',\n",
       "  'put',\n",
       "  'my',\n",
       "  'computer',\n",
       "  'games',\n",
       "  'on',\n",
       "  'so',\n",
       "  'my',\n",
       "  'computer',\n",
       "  'would',\n",
       "  'run',\n",
       "  'faster',\n",
       "  'and',\n",
       "  'i',\n",
       "  'could',\n",
       "  'play',\n",
       "  'more',\n",
       "  'than',\n",
       "  'one',\n",
       "  'game',\n",
       "  'at',\n",
       "  'a',\n",
       "  'time']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_words = list()\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    if i >= 800000 and i < 1000000:\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        sent_list = list()\n",
    "        sents = sent_detector.tokenize(review)\n",
    "        for sent in sents:\n",
    "            word_list = list()\n",
    "            words = word_tok.tokenize(sent)\n",
    "            for word in words:\n",
    "                word = re.sub(r'[^\\w\\s]|[0-9]', '', word).lower().strip()\n",
    "                if word:\n",
    "                    if check_vocab and en_dict.check(word):\n",
    "                        word_list.append(word)\n",
    "                    elif remove_rare and word not in rare_words:\n",
    "                        word_list.append(word)\n",
    "                    else:\n",
    "                        word_list.append(word)\n",
    "            if word_list:\n",
    "                sent_list.append(word_list)\n",
    "        if sent_list:\n",
    "            review_words.append(sent_list)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.4.pkl'), 'wb') as f:\n",
    "    pickle.dump(review_words, f)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.4.pkl'), 'rb') as f:\n",
    "    r = pickle.load(f)\n",
    "    \n",
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6324883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'sound',\n",
       "  'is',\n",
       "  'good',\n",
       "  'but',\n",
       "  'the',\n",
       "  'ear',\n",
       "  'pieces',\n",
       "  'are',\n",
       "  'too',\n",
       "  'big',\n",
       "  'for',\n",
       "  'my',\n",
       "  'ears',\n",
       "  'and',\n",
       "  'are',\n",
       "  'not',\n",
       "  'comfortable'],\n",
       " ['if',\n",
       "  'you',\n",
       "  'want',\n",
       "  'comfort',\n",
       "  'i',\n",
       "  'would',\n",
       "  'recommend',\n",
       "  'another',\n",
       "  'kind'],\n",
       " ['if', 'you', 'want', 'sound', 'these', 'were', 'great']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_words = list()\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    if i >= 1000000 and i < 1200000:\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        sent_list = list()\n",
    "        sents = sent_detector.tokenize(review)\n",
    "        for sent in sents:\n",
    "            word_list = list()\n",
    "            words = word_tok.tokenize(sent)\n",
    "            for word in words:\n",
    "                word = re.sub(r'[^\\w\\s]|[0-9]', '', word).lower().strip()\n",
    "                if word:\n",
    "                    if check_vocab and en_dict.check(word):\n",
    "                        word_list.append(word)\n",
    "                    elif remove_rare and word not in rare_words:\n",
    "                        word_list.append(word)\n",
    "                    else:\n",
    "                        word_list.append(word)\n",
    "            if word_list:\n",
    "                sent_list.append(word_list)\n",
    "        if sent_list:\n",
    "            review_words.append(sent_list)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.5.pkl'), 'wb') as f:\n",
    "    pickle.dump(review_words, f)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.5.pkl'), 'rb') as f:\n",
    "    r = pickle.load(f)\n",
    "    \n",
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b55750e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'have',\n",
       "  'bought',\n",
       "  'a',\n",
       "  'bluerigger',\n",
       "  'cable',\n",
       "  'before',\n",
       "  'for',\n",
       "  'my',\n",
       "  'tv',\n",
       "  'and',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'quality',\n",
       "  'product'],\n",
       " ['ten',\n",
       "  'feet',\n",
       "  'is',\n",
       "  'plenty',\n",
       "  'of',\n",
       "  'cable',\n",
       "  'to',\n",
       "  'connect',\n",
       "  'a',\n",
       "  'camera',\n",
       "  'etc',\n",
       "  'to',\n",
       "  'an',\n",
       "  'hdmi',\n",
       "  'tv'],\n",
       " ['i',\n",
       "  'use',\n",
       "  'the',\n",
       "  'cable',\n",
       "  'to',\n",
       "  'link',\n",
       "  'my',\n",
       "  'canon',\n",
       "  'eos',\n",
       "  'rebel',\n",
       "  't',\n",
       "  'to',\n",
       "  'my',\n",
       "  'tv']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_words = list()\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    if i >= 1200000 and i < 1400000:\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        sent_list = list()\n",
    "        sents = sent_detector.tokenize(review)\n",
    "        for sent in sents:\n",
    "            word_list = list()\n",
    "            words = word_tok.tokenize(sent)\n",
    "            for word in words:\n",
    "                word = re.sub(r'[^\\w\\s]|[0-9]', '', word).lower().strip()\n",
    "                if word:\n",
    "                    if check_vocab and en_dict.check(word):\n",
    "                        word_list.append(word)\n",
    "                    elif remove_rare and word not in rare_words:\n",
    "                        word_list.append(word)\n",
    "                    else:\n",
    "                        word_list.append(word)\n",
    "            if word_list:\n",
    "                sent_list.append(word_list)\n",
    "        if sent_list:\n",
    "            review_words.append(sent_list)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.6.pkl'), 'wb') as f:\n",
    "    pickle.dump(review_words, f)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.6.pkl'), 'rb') as f:\n",
    "    r = pickle.load(f)\n",
    "    \n",
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d8e45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['put',\n",
       "  'the',\n",
       "  'mids',\n",
       "  'in',\n",
       "  'my',\n",
       "  'sons',\n",
       "  'honda',\n",
       "  'and',\n",
       "  'his',\n",
       "  'response',\n",
       "  'those',\n",
       "  'speakers',\n",
       "  'sound',\n",
       "  'amazing',\n",
       "  'ive',\n",
       "  'always',\n",
       "  'liked',\n",
       "  'rf',\n",
       "  'products',\n",
       "  'and',\n",
       "  'this',\n",
       "  'is',\n",
       "  'a',\n",
       "  'step',\n",
       "  'up',\n",
       "  'from',\n",
       "  'the',\n",
       "  'last',\n",
       "  'punch',\n",
       "  'set',\n",
       "  'i',\n",
       "  'boughtpros',\n",
       "  'perfect',\n",
       "  'fit',\n",
       "  'after',\n",
       "  'breaking',\n",
       "  'off',\n",
       "  'the',\n",
       "  'tabs',\n",
       "  'suggested',\n",
       "  'in',\n",
       "  'the',\n",
       "  'instructions',\n",
       "  'for',\n",
       "  'certain',\n",
       "  'cars',\n",
       "  'good',\n",
       "  'bass',\n",
       "  'and',\n",
       "  'solid',\n",
       "  'midrangecons',\n",
       "  'a',\n",
       "  'tad',\n",
       "  'power',\n",
       "  'hungry',\n",
       "  'but',\n",
       "  'certainly',\n",
       "  'not',\n",
       "  'a',\n",
       "  'deal',\n",
       "  'killer']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_words = list()\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    if i >= 1400000:\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        sent_list = list()\n",
    "        sents = sent_detector.tokenize(review)\n",
    "        for sent in sents:\n",
    "            word_list = list()\n",
    "            words = word_tok.tokenize(sent)\n",
    "            for word in words:\n",
    "                word = re.sub(r'[^\\w\\s]|[0-9]', '', word).lower().strip()\n",
    "                if word:\n",
    "                    if check_vocab and en_dict.check(word):\n",
    "                        word_list.append(word)\n",
    "                    elif remove_rare and word not in rare_words:\n",
    "                        word_list.append(word)\n",
    "                    else:\n",
    "                        word_list.append(word)\n",
    "            if word_list:\n",
    "                sent_list.append(word_list)\n",
    "        if sent_list:\n",
    "            review_words.append(sent_list)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.7.pkl'), 'wb') as f:\n",
    "    pickle.dump(review_words, f)\n",
    "    \n",
    "with open(os.path.join(store_dir, filename + '.7.pkl'), 'rb') as f:\n",
    "    r = pickle.load(f)\n",
    "    \n",
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77adf0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening ./rarity-checked/review.words.unchecked.rarity.1.pkl  Contains 169921 entries\n",
      "Opening ./rarity-checked/review.words.unchecked.rarity.2.pkl  Contains 229883 entries\n",
      "Opening ./rarity-checked/review.words.unchecked.rarity.3.pkl  Contains 199829 entries\n",
      "Opening ./rarity-checked/review.words.unchecked.rarity.4.pkl  Contains 199862 entries\n",
      "Opening ./rarity-checked/review.words.unchecked.rarity.5.pkl  Contains 199863 entries\n",
      "Opening ./rarity-checked/review.words.unchecked.rarity.6.pkl  Contains 199900 entries\n",
      "Opening ./rarity-checked/review.words.unchecked.rarity.7.pkl  Contains 288993 entries\n",
      "Loading data Time taken: 61.52171039581299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1488251"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "reviews = list()\n",
    "\n",
    "for i in range(7):\n",
    "    filepath = os.path.join(store_dir, filename + '.' + str(i+1) + '.pkl')\n",
    "    print('Opening', filepath, end='  ')\n",
    "    with open(filepath, 'rb') as f:\n",
    "        reviews_set = pickle.load(f)\n",
    "        print('Contains', len(reviews_set), 'entries')\n",
    "        reviews.extend(reviews_set)\n",
    "\n",
    "end = time.time()\n",
    "print('Loading data', 'Time taken:', end - start)\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41e4b98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count Time taken: 52.20235466957092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "860707"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "word_count = dict()\n",
    "SLICE = 10000\n",
    "\n",
    "for review in reviews[:]:\n",
    "    for sentence in review:\n",
    "        for word in sentence:\n",
    "            if not word in word_count:\n",
    "                word_count[word] = 1\n",
    "            else:\n",
    "                word_count[word] += 1\n",
    "                \n",
    "vocab_size = len(word_count)\n",
    "end = time.time()\n",
    "print('Word count', 'Time taken:', end - start)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c4ca4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rare_words = set()\n",
    "\n",
    "# for word in word_count.keys():\n",
    "#     if word_count[word] < 7:\n",
    "#         rare_words.add(word)\n",
    "\n",
    "# rare_words = list(rare_words)\n",
    "\n",
    "# with open('unchecked_vocab.pkl', 'wb') as f:\n",
    "#     pickle.dump(list(word_count.keys()), f)\n",
    "\n",
    "# with open('rare_words.pkl', 'wb') as f:\n",
    "#     pickle.dump(rare_words, f)\n",
    "\n",
    "# print(len(rare_words))\n",
    "# print(list(rare_words)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67274a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = set()\n",
    "\n",
    "# for word in word_count.keys():\n",
    "#     if word not in rare_words:\n",
    "#         vocab.add(word)\n",
    "        \n",
    "# vocab = list(vocab)\n",
    "# vocab_size = len(vocab)\n",
    "# print(vocab_size)\n",
    "# print(vocab[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "562c4c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "860707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'got',\n",
       " 'this',\n",
       " 'gps',\n",
       " 'for',\n",
       " 'my',\n",
       " 'husband',\n",
       " 'who',\n",
       " 'is',\n",
       " 'an',\n",
       " 'otr',\n",
       " 'over',\n",
       " 'the',\n",
       " 'road',\n",
       " 'trucker',\n",
       " 'very',\n",
       " 'impressed',\n",
       " 'with',\n",
       " 'shipping',\n",
       " 'time',\n",
       " 'it',\n",
       " 'arrived',\n",
       " 'a',\n",
       " 'few',\n",
       " 'days',\n",
       " 'earlier',\n",
       " 'than',\n",
       " 'expected',\n",
       " 'within',\n",
       " 'week',\n",
       " 'of',\n",
       " 'use',\n",
       " 'however',\n",
       " 'started',\n",
       " 'freezing',\n",
       " 'up',\n",
       " 'could',\n",
       " 'just',\n",
       " 'been',\n",
       " 'glitch',\n",
       " 'in',\n",
       " 'that',\n",
       " 'unit',\n",
       " 'worked',\n",
       " 'great',\n",
       " 'when',\n",
       " 'will',\n",
       " 'work',\n",
       " 'normal',\n",
       " 'person',\n",
       " 'as',\n",
       " 'well',\n",
       " 'but',\n",
       " 'does',\n",
       " 'have',\n",
       " 'option',\n",
       " 'big',\n",
       " 'truck',\n",
       " 'routes',\n",
       " 'tells',\n",
       " 'you',\n",
       " 'scale',\n",
       " 'coming',\n",
       " 'ect',\n",
       " 'love',\n",
       " 'bigger',\n",
       " 'screen',\n",
       " 'ease',\n",
       " 'putting',\n",
       " 'addresses',\n",
       " 'into',\n",
       " 'memory',\n",
       " 'nothing',\n",
       " 'really',\n",
       " 'bad',\n",
       " 'to',\n",
       " 'say',\n",
       " 'about',\n",
       " 'exception',\n",
       " 'which',\n",
       " 'probably',\n",
       " 'one',\n",
       " 'million',\n",
       " 'and',\n",
       " 'thats',\n",
       " 'luck',\n",
       " 'i',\n",
       " 'contacted',\n",
       " 'seller',\n",
       " 'minutes',\n",
       " 'email',\n",
       " 'received',\n",
       " 'back',\n",
       " 'instructions',\n",
       " 'exchange',\n",
       " 'all',\n",
       " 'way',\n",
       " 'around',\n",
       " 'im',\n",
       " 'professional']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(word_count.keys())\n",
    "\n",
    "with open('svd_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "    \n",
    "print(len(vocab))\n",
    "vocab[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9041885e",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.39 TiB for an array with shape (860707, 860707) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4d4c3fcc07d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mco_occ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mSLICE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 5.39 TiB for an array with shape (860707, 860707) and data type float64"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "co_occ = np.zeros((vocab_size, vocab_size))\n",
    "\n",
    "for i, review in enumerate(reviews[:SLICE]):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    for sentence in review:\n",
    "        sent_len = len(sentence)\n",
    "        for wi in range(sent_len):\n",
    "            curr_word = sentence[wi]\n",
    "            curr_ind = vocab.index(curr_word)\n",
    "\n",
    "            left_ind = max(0, wi-3)\n",
    "            right_ind = min(sent_len, wi+4)\n",
    "\n",
    "            for cont_word_ind in range(left_ind, right_ind):\n",
    "                cont_word = sentence[cont_word_ind]\n",
    "                cont_ind = vocab.index(cont_word)\n",
    "                co_occ[curr_ind, cont_ind] += 1\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end - start)\n",
    "co_occ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051d7f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 99.1198100239618\n",
      "Time taken: 37.55903673171997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<50932x50932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 22832732 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "sparsity = (1 - np.count_nonzero(co_occ) / co_occ.size) * 100\n",
    "print('Sparsity:', sparsity)\n",
    "\n",
    "co_occ_csr = sp.csr_matrix(co_occ)\n",
    "end = time.time()\n",
    "\n",
    "print('Time taken:', end - start)\n",
    "co_occ_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ebd83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "u, s, vt = sp.linalg.svds(co_occ_csr, k=750, which='LM')\n",
    "end = time.time()\n",
    "\n",
    "print('Time taken:', end - start)\n",
    "print(u.shape, s.shape, vt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(u * s, vt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7694ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(co_occ - np.matmul(u * s, vt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.save_npz('co_occ.npz', co_occ_csr)\n",
    "np.save('u', u)\n",
    "np.save('s', s)\n",
    "np.save('vt', vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occ_csr = sp.load_npz('co_occ.npz')\n",
    "u = np.load('u.npy')\n",
    "s = np.load('s.npy')\n",
    "vt = np.load('vt.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c00af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(co_occ - np.matmul(u * s, vt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19c9b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occ = sp.load_npz('co_occ.npz').toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870f7cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
